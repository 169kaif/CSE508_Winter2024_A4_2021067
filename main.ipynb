{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of             Id   ProductId          UserId                      ProfileName  \\\n",
      "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "...        ...         ...             ...                              ...   \n",
      "568449  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
      "568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
      "568451  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
      "568452  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
      "568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
      "\n",
      "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                          1                       1      5  1303862400   \n",
      "1                          0                       0      1  1346976000   \n",
      "2                          1                       1      4  1219017600   \n",
      "3                          3                       3      2  1307923200   \n",
      "4                          0                       0      5  1350777600   \n",
      "...                      ...                     ...    ...         ...   \n",
      "568449                     0                       0      5  1299628800   \n",
      "568450                     0                       0      2  1331251200   \n",
      "568451                     2                       2      5  1329782400   \n",
      "568452                     1                       1      5  1331596800   \n",
      "568453                     0                       0      5  1338422400   \n",
      "\n",
      "                                   Summary  \\\n",
      "0                    Good Quality Dog Food   \n",
      "1                        Not as Advertised   \n",
      "2                    \"Delight\" says it all   \n",
      "3                           Cough Medicine   \n",
      "4                              Great taffy   \n",
      "...                                    ...   \n",
      "568449                 Will not do without   \n",
      "568450                        disappointed   \n",
      "568451            Perfect for our maltipoo   \n",
      "568452  Favorite Training and reward treat   \n",
      "568453                         Great Honey   \n",
      "\n",
      "                                                     Text  \n",
      "0       I have bought several of the Vitality canned d...  \n",
      "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2       This is a confection that has been around a fe...  \n",
      "3       If you are looking for the secret ingredient i...  \n",
      "4       Great taffy at a great price.  There was a wid...  \n",
      "...                                                   ...  \n",
      "568449  Great for sesame chicken..this is a good if no...  \n",
      "568450  I'm disappointed with the flavor. The chocolat...  \n",
      "568451  These stars are small, so you can give 10-15 o...  \n",
      "568452  These are the BEST treats for training and rew...  \n",
      "568453  I am very satisfied ,product is as advertised,...  \n",
      "\n",
      "[568401 rows x 10 columns]>\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "\n",
    "data_fp = \"Reviews.csv\"\n",
    "\n",
    "df = pd.read_csv(data_fp, on_bad_lines=\"skip\")\n",
    "df.dropna(inplace=True)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    #remove the html tags / links\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    #remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    #lemmatize the text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    #normalize the text\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the 'Text' and 'Summary' columns and store them in new columns 'clean_text' and 'clean_summary'\n",
    "\n",
    "df['clean_text'] = df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_summary'] = df['Summary'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick a subset of the dataframe to work with\n",
    "\n",
    "df = df.sample(3200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make train-test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['clean_text'].tolist()\n",
    "y = df['clean_summary'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tokenizer and model\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", pad_token=\"<|endoftext|>\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement custom dataset class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SummaryDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = 100\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoded_input = self.prepare_sequence(text)\n",
    "        encoded_label = self.prepare_sequence(label)\n",
    "\n",
    "        return encoded_input, encoded_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def prepare_sequence(self, text):\n",
    "        # tokenize the text\n",
    "        tokenized_text = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "        # pad and truncate the tokenized text\n",
    "        if len(tokenized_text) < self.max_length:\n",
    "            padded_text = tokenized_text + [self.tokenizer.pad_token_id] * (self.max_length - len(tokenized_text))\n",
    "        else:\n",
    "            padded_text = tokenized_text[:self.max_length]\n",
    "\n",
    "        return torch.tensor(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize custom dataset class into a dataloader\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = SummaryDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = SummaryDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load optimizer and loss function\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "#send model to device\n",
    "model = model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_epochs(model, optimizer, train_loader):\n",
    "\n",
    "    # train and evaluate the model\n",
    "    num_epochs = 3\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "\n",
    "            #zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_batch, label_batch = batch\n",
    "            input_batch = input_batch.to(\"mps\")\n",
    "            label_batch = label_batch.to(\"mps\")\n",
    "\n",
    "            #forward pass to calculate loss\n",
    "            outputs = model(input_batch)\n",
    "\n",
    "            logits = outputs[0]\n",
    "\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), label_batch.view(-1))\n",
    "\n",
    "            #backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            torch.mps.empty_cache()\n",
    "        \n",
    "        # calculate average train_loss for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1649 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Epochs: 100%|██████████| 3/3 [10:02<00:00, 200.69s/it]\n"
     ]
    }
   ],
   "source": [
    "# run the training loop\n",
    "\n",
    "run_epochs(model, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "torch.save(model.state_dict(), \"gpt2_finetuned.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
